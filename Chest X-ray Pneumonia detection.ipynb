{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6efcfb-37d5-4517-b73b-5a52416e83df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\amit\\anaconda3\\lib\\site-packages (3.1.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Amit\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-g0c2ptbx\\\\h5.cp38-win_amd64.pyd'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.6.0-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\amit\\anaconda3\\lib\\site-packages (from h5py) (1.20.1)\n",
      "Installing collected packages: h5py\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cd4934-583f-4f5f-87d5-3f1810b8dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing to liberaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, BatchNormalization, Flatten, Dropout\n",
    "import cv2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66430d6-7cfd-4cfe-8ca8-127efb84e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "train_dir = \"C:\\\\Users\\\\Amit\\\\Desktop\\\\ML Projects\\\\Chest X-ray Pnumonia\\\\Data\\\\train\"\n",
    "test_dir = \"C:\\\\Users\\\\Amit\\\\Desktop\\\\ML Projects\\\\Chest X-ray Pnumonia\\\\Data\\\\test\"\n",
    "val_dir = \"C:\\\\Users\\\\Amit\\\\Desktop\\\\ML Projects\\\\Chest X-ray Pnumonia\\\\Data\\\\val\"\n",
    "categories = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2760efb-9634-4a73-8b12-001e1f03fc5f",
   "metadata": {},
   "source": [
    "# Extract to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b17bb8-4528-4848-82b2-4fb6f4a5c62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0115-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0117-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0119-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0122-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0125-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0115-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0117-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0119-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0122-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0125-0001.jpeg   \n",
       "\n",
       "  Category  \n",
       "0   NORMAL  \n",
       "1   NORMAL  \n",
       "2   NORMAL  \n",
       "3   NORMAL  \n",
       "4   NORMAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Training df with file path, fine name, and class as variables\n",
    "paths= []\n",
    "category_list=[]\n",
    "file = []\n",
    "for cate in categories:\n",
    "    path = os.path.join(train_dir, cate)\n",
    "    \n",
    "    for img in os.listdir(path):\n",
    "        paths.append(os.path.abspath(img))\n",
    "        category_list.append(cate)\n",
    "        file.append(img)\n",
    "        \n",
    "df_train = pd.DataFrame({'Paths' :paths, 'File':file ,'Category':category_list})\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04aa3a-400c-4342-b42a-5d159cba3c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b13b3b-ac36-40f9-9a8b-dc491964594b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0001-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0003-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0005-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0006-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0007-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0001-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0003-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0005-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0006-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0007-0001.jpeg   \n",
       "\n",
       "  Category  \n",
       "0   NORMAL  \n",
       "1   NORMAL  \n",
       "2   NORMAL  \n",
       "3   NORMAL  \n",
       "4   NORMAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Test df with file path, fine name, and class as variables\n",
    "paths= []\n",
    "category_list=[]\n",
    "file = []\n",
    "for cate in categories:\n",
    "    path = os.path.join(test_dir, cate)\n",
    "    \n",
    "    for img in os.listdir(path):\n",
    "        paths.append(os.path.abspath(img))\n",
    "        category_list.append(cate)\n",
    "        file.append(img)\n",
    "        \n",
    "df_test = pd.DataFrame({'Paths' :paths, 'File':file ,'Category':category_list})\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99ea466-bf40-4b70-aae2-c70c7a3f5d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0335-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0339-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0377-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0740-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>NORMAL2-IM-0439-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...   \n",
       "\n",
       "                        File Category  \n",
       "0          IM-0335-0001.jpeg   NORMAL  \n",
       "1          IM-0339-0001.jpeg   NORMAL  \n",
       "2          IM-0377-0001.jpeg   NORMAL  \n",
       "3          IM-0740-0001.jpeg   NORMAL  \n",
       "4  NORMAL2-IM-0439-0001.jpeg   NORMAL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Validation df with file path, fine name, and class as variables\n",
    "paths= []\n",
    "category_list=[]\n",
    "file = []\n",
    "for cate in categories:\n",
    "    path = os.path.join(val_dir, cate)\n",
    "    \n",
    "    for img in os.listdir(path):\n",
    "        paths.append(os.path.abspath(img))\n",
    "        category_list.append(cate)\n",
    "        file.append(img)\n",
    "        \n",
    "df_val = pd.DataFrame({'Paths' :paths, 'File':file ,'Category':category_list})\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04eeae-4fca-4fc1-a1f7-f5f6d7ea9fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b7f71eb-7150-41d9-b452-62fc1ad4ec23",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbeec746-d0dd-45c8-a450-fcb9f8522f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "image_size = 450\n",
    "input_shape = (450,450,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ab04a-96bc-4fb0-aa9e-3e48e6ae5485",
   "metadata": {},
   "source": [
    "#### Train set generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d15879d6-ea9e-4c28-9ed1-ed188a6be174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Generator to input data into the model\n",
    "# Performing some data auigmentation as well\n",
    "\n",
    "train = ImageDataGenerator(rescale=1/155,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c19235a-39bb-4f45-be61-e69dcfeb04b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NORMAL': 0, 'PNEUMONIA': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHecking the 2 class catagories\n",
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e2549-ccc7-4b6b-b165-625a701bf129",
   "metadata": {},
   "source": [
    "#### Val set generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da015e9-cba0-4ef6-9d18-e2045ecf23c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Generator to input data into the model\n",
    "# Performing some data auigmentation as well\n",
    "\n",
    "valid = ImageDataGenerator(rescale=1/155)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ef68e-4e24-4cd4-be99-605fcfdb40fb",
   "metadata": {},
   "source": [
    "#### Test set generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365b1171-c3a4-4ebb-abf4-dab08b378d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Generator to input data into the model\n",
    "# Performing some data auigmentation as well\n",
    "\n",
    "test = ImageDataGenerator(rescale=1/155)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659067c-3d0d-4ecf-83e7-e34cf1dcb22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd44e91e-8130-4bd6-8603-ffd150e87ee2",
   "metadata": {},
   "source": [
    "#### Call Back Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7207f2b7-b485-4db3-ac5c-a4a6b036e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs for early stoping\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ccdf10-50b3-490b-a9cd-66762a4e9479",
   "metadata": {},
   "source": [
    "# Model building\n",
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0364ebc-5a31-4a93-83ab-72ef1926d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 450, 450, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 450, 450, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 225, 225, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 225, 225, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 225, 225, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 225, 225, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 112, 112, 512)     33280     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6422528)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 12845058  \n",
      "=================================================================\n",
      "Total params: 12,897,538\n",
      "Trainable params: 12,897,346\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af9889a0-440a-43e8-81ca-3995afd13d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "260/260 [==============================] - 83s 285ms/step - loss: 18.5320 - accuracy: 0.8970 - val_loss: 11.8898 - val_accuracy: 0.6042\n",
      "Epoch 2/100\n",
      "260/260 [==============================] - 70s 270ms/step - loss: 1.1837 - accuracy: 0.9383 - val_loss: 2.9353 - val_accuracy: 0.8125\n",
      "Epoch 3/100\n",
      "260/260 [==============================] - 69s 265ms/step - loss: 0.3480 - accuracy: 0.9473 - val_loss: 1.1571 - val_accuracy: 0.8125\n",
      "Epoch 4/100\n",
      "260/260 [==============================] - 70s 269ms/step - loss: 0.1999 - accuracy: 0.9560 - val_loss: 1.4383 - val_accuracy: 0.8125\n",
      "Epoch 5/100\n",
      "260/260 [==============================] - 70s 269ms/step - loss: 0.1429 - accuracy: 0.9570 - val_loss: 6.9081 - val_accuracy: 0.5417\n",
      "Epoch 6/100\n",
      "260/260 [==============================] - 70s 269ms/step - loss: 0.1157 - accuracy: 0.9620 - val_loss: 0.9301 - val_accuracy: 0.7917\n",
      "Epoch 7/100\n",
      "260/260 [==============================] - 70s 268ms/step - loss: 0.1095 - accuracy: 0.9633 - val_loss: 15.4055 - val_accuracy: 0.5417\n",
      "Epoch 8/100\n",
      "260/260 [==============================] - 70s 269ms/step - loss: 0.1128 - accuracy: 0.9624 - val_loss: 8.1569 - val_accuracy: 0.6250\n",
      "Epoch 9/100\n",
      "260/260 [==============================] - 70s 269ms/step - loss: 0.1160 - accuracy: 0.9616 - val_loss: 4.8021 - val_accuracy: 0.6875\n",
      "Epoch 10/100\n",
      "260/260 [==============================] - 70s 271ms/step - loss: 0.0749 - accuracy: 0.9732 - val_loss: 1.3914 - val_accuracy: 0.8333\n",
      "Epoch 11/100\n",
      "260/260 [==============================] - 70s 269ms/step - loss: 0.0774 - accuracy: 0.9691 - val_loss: 2.1797 - val_accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b89777-ef9d-4ba4-aa50-e0509917f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd05e3-06d1-4a2c-9fab-34b422111d1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4ab0b-885c-492e-b30a-9c5be39a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45db1a-b933-47b0-8412-34233c9478dd",
   "metadata": {},
   "source": [
    "#### Pred df model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b0d4b-5df4-414c-9b32-6d17fcfb2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99076f-0f33-46e5-b8d8-6612020c4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_pred = df_test.copy()\n",
    "\n",
    "model1_pred['actual'] = model1_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model1_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model1_pred['class_pred'] = model1_pred['Pred_proba'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model1_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00d2c8-0593-4798-8ad6-f973dc7b6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model1_pred['actual'], model1_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274afbe-a47a-46a8-8543-4a294901f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model1_pred['actual'], model1_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c8923-d1b9-4f07-88aa-e1829d24180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5b46d-9b14-4732-8682-04f62caf790f",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "- Increasing image size to 500 pixels\n",
    "- adding a 16 layer convolution at the beginning of the cnn\n",
    "- adding another dense layer of 1024\n",
    "- will increase batch size to see what fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934aa71b-6af3-4394-982b-6591069674ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 500\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab4b9b-ef2e-4705-a680-60fc9bc724ad",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90b960-434f-47d9-a3a6-e323faedb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale=1/155,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28630c44-63e8-42e2-aada-8c82e2a4b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ImageDataGenerator(rescale=1/155)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f7453-de0b-4ce9-9595-641585aed8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageDataGenerator(rescale=1/155)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb41ba-b831-4d0d-8682-c28f68c634cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eb67f-a6b7-467f-a765-998131d596cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db7e21-b4bc-4e14-8384-d607f1312077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0faadaf-39aa-49f6-ae3b-51d0875063ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33874b-6bb5-4fad-beac-05707c67446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497acc7-dc3b-4414-b43e-cc855df2d1c7",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070002c-9851-4a8b-932f-4160a4256256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b370a-2ef3-4733-8c55-fe973aafb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773fb3d-dde3-4642-93a1-6516c381ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_pred = df_test.copy()\n",
    "\n",
    "model2_pred['actual'] = model2_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model2_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model2_pred['class_pred'] = model2_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model2_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d073f-e8f8-4ccd-bdbb-6392a75817fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(model2_pred['actual'], model2_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990e390-145a-442a-8a02-37099b23fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model2_pred['actual'], model2_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3afbc-ca0b-42e7-b4da-ec0184d1a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model.save('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7213091-cbb1-4fdf-9765-27b940fbc777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3dae175-d6c9-4d3c-be5d-44ce095b92cc",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "### ALso just realized that the rescaling factor was 155. It should be 255. Changing that. Ooops\n",
    "- Decreasing Batch size seemed to increase model performance.\n",
    "- Will change batch size to 8\n",
    "- Adding anotheer conv layer of 128\n",
    "- will increase batch size to see what fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ae4b9-3b45-4513-934f-8b0cb2cbcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "image_size = 500\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a39cb3-96f7-4fd3-a3e6-a960ec0cfc47",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff903b-d12b-48b9-97d1-d45b3b25f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df8b50-d429-4d95-9e70-03b017f56d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180a02b-e161-4344-b66b-77586b173c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6ebe6-5edc-4f82-bcc4-d088b5a9eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e09c11-142b-4f3c-97f3-af28dd420bf6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668dab9-8591-4f64-9010-d0e60c7a5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f27c8-a163-4e00-9e18-314c7bbc20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43441ac2-33de-4636-a139-6b459be5adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a4954-556b-4571-9642-43b0a4e04fcf",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ea9bb-c5d1-4227-8716-01c9a0e6dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca9b06-cf56-4d0b-93a7-632a8ffa9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc63532-5742-429e-b632-325af15a4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_pred = df_test.copy()\n",
    "\n",
    "model3_pred['actual'] = model3_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model3_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model3_pred['class_pred'] = model3_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model3_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4528e4b-b941-4ba9-9200-54e5ba5b19d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(model3_pred['actual'], model3_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b894f-99cd-4663-914a-5ceecbe232e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model3_pred['actual'], model3_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd124ac-178d-4833-95cc-383b76b8038e",
   "metadata": {},
   "source": [
    "- So it seems like complex models perform poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625cafb-be67-4390-814a-581fde476290",
   "metadata": {},
   "source": [
    "# Model 4\n",
    "- Will build a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc1913-58b8-4e6e-90f4-a5f6d6cba74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "image_size = 500\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05be9c-dc0c-4c59-8546-a5e83dfc649f",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69ff3c-ba98-4385-9b08-283fed6b83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b148fbe-2d46-4558-a28c-fb7ba78c3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e8e60-b812-4e99-b9b6-2cf68ec914f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c88af-5050-4e9e-a465-8c4aca36dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1685711-b5a3-48a9-9322-fa1158b513c4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58fd1f-5b0b-4ac4-b4da-eb3012396a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fbb4af-834f-44de-b486-fdc44ec7d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091feb8-96a1-45d4-8e8a-b447ee14e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852683b-621c-46da-9b37-0df0904c7590",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ac119-2721-453c-a550-3ad1b5feac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ee490-2aa8-4862-a654-ead0b5ddb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc79ca-6629-4b49-b6cc-c7768815e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_pred = df_test.copy()\n",
    "\n",
    "model4_pred['actual'] = model4_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model4_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model4_pred['class_pred'] = model4_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model4_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb394d-f7be-470d-af39-bfc4aa030082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(model4_pred['actual'], model4_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e4361-444c-4b1a-a1ef-f19c34fda138",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model4_pred['actual'], model4_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f188a8-821b-43a8-ae11-bea0bc5ef700",
   "metadata": {},
   "source": [
    "- Seemes like a simple model works better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0bfec-9401-4d21-97f5-b4e16fad2f61",
   "metadata": {},
   "source": [
    "# Model 5\n",
    "- Looked up models online\n",
    "- Getting performance of 90+%\n",
    "- They are using a bit more complex models but a smaller image size\n",
    "- Trying image size of 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ea6813-693d-427f-a0d8-902e8680b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 250\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64dd03-2a85-431c-886f-f587e026eaa4",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2d34ab-fc2e-4a83-b23a-2127b1321e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f4d24f-b695-441c-9c35-168a2ed924d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212281bf-8da8-41a4-b124-e650d3897cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c6e748-c8e1-4891-8028-7be4d9191755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9158d2-45ca-483d-b0bc-4e51c4fb4157",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82904d6-8963-44d6-8eaa-95f4f3d81d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 250, 250, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 250, 250, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 125, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 62, 62, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 31, 31, 512)       33280     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31, 31, 1024)      525312    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 984064)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1968130   \n",
      "=================================================================\n",
      "Total params: 2,550,466\n",
      "Trainable params: 2,550,242\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d133df02-6b3a-4ae9-b5e7-124ce3d55139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "324/324 [==============================] - 42s 107ms/step - loss: 0.5431 - accuracy: 0.9107 - val_loss: 6.8163 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.1221 - accuracy: 0.9554 - val_loss: 4.3944 - val_accuracy: 0.5417\n",
      "Epoch 3/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.1232 - accuracy: 0.9541 - val_loss: 0.6050 - val_accuracy: 0.8333\n",
      "Epoch 4/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.1037 - accuracy: 0.9632 - val_loss: 0.9241 - val_accuracy: 0.8125\n",
      "Epoch 5/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.1149 - accuracy: 0.9583 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
      "Epoch 6/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.1071 - accuracy: 0.9622 - val_loss: 0.5427 - val_accuracy: 0.8333\n",
      "Epoch 7/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.1045 - accuracy: 0.9618 - val_loss: 1.0069 - val_accuracy: 0.6250\n",
      "Epoch 8/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0986 - accuracy: 0.9643 - val_loss: 0.3782 - val_accuracy: 0.7917\n",
      "Epoch 9/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.1001 - accuracy: 0.9664 - val_loss: 1.1549 - val_accuracy: 0.7292\n",
      "Epoch 10/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0911 - accuracy: 0.9653 - val_loss: 0.4132 - val_accuracy: 0.8542\n",
      "Epoch 11/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0908 - accuracy: 0.9695 - val_loss: 0.4572 - val_accuracy: 0.8542\n",
      "Epoch 12/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0657 - accuracy: 0.9774 - val_loss: 0.4108 - val_accuracy: 0.8542\n",
      "Epoch 13/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0627 - accuracy: 0.9769 - val_loss: 0.6333 - val_accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc783179-0c16-4c76-9e09-df548e9b36a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x247e2d414c0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ab606-14c6-4459-a255-7d7cd201e468",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d869037-e1ac-4ce6-85fe-91381ae3cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b65249-b573-4668-be37-77c97f7480e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ce532b-8c28-444e-9546-8aebc3bdfc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "      <th>actual</th>\n",
       "      <th>Pred_proba_Normal</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0001-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0003-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.349138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0005-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.449169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0006-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.742099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0007-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0001-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0003-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0005-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0006-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0007-0001.jpeg   \n",
       "\n",
       "  Category  actual  Pred_proba_Normal  class_pred  \n",
       "0   NORMAL       0           0.967120           0  \n",
       "1   NORMAL       0           0.349138           1  \n",
       "2   NORMAL       0           0.449169           1  \n",
       "3   NORMAL       0           0.742099           0  \n",
       "4   NORMAL       0           0.988462           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5_pred = df_test.copy()\n",
    "\n",
    "model5_pred['actual'] = model5_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model5_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model5_pred['class_pred'] = model5_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model5_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fcefd7e-827c-4d57-ac29-c5afa77af3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[110, 124],\n",
       "       [  4, 386]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(model5_pred['actual'], model5_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c132149-0e7c-43f8-9e6e-4ee67a908620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7948717948717948"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model5_pred['actual'], model5_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbf6ab-900b-4da0-9457-a2147576583d",
   "metadata": {},
   "source": [
    "- model performed average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623a2d0-5bd0-4f50-bfa6-75839e1b1949",
   "metadata": {},
   "source": [
    "# Model 6\n",
    "- Will try a bit  more complex model\n",
    "- will add one more layer of cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ead62e-2047-4cc7-83c3-cd89e66f6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 250\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c93988-5981-407b-8da8-d3ec13fefbc3",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0c3ecbd-c8c8-4c3e-9611-251408a12142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "774e5dac-cb6d-4f9f-a6b0-5b9e85758304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "985fc2b7-83af-47c5-9e4d-83d647573898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd1769e-bd50-4780-a051-a5fa9e5f995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27623be-f6f2-427f-90b6-553e1b34ae16",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe187731-9de2-43b7-813b-3d6a3a3b6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 250, 250, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 250, 250, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 125, 125, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 62, 62, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 31, 31, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15, 15, 512)       66048     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15, 15, 1024)      525312    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 230400)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 460802    \n",
      "=================================================================\n",
      "Total params: 1,150,274\n",
      "Trainable params: 1,149,794\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c83b1677-a646-4c2d-9315-6e97ca308b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "324/324 [==============================] - 31s 93ms/step - loss: 0.3912 - accuracy: 0.9068 - val_loss: 3.0572 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.1363 - accuracy: 0.9491 - val_loss: 0.3680 - val_accuracy: 0.8333\n",
      "Epoch 3/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.1287 - accuracy: 0.9504 - val_loss: 0.5611 - val_accuracy: 0.6875\n",
      "Epoch 4/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.1106 - accuracy: 0.9579 - val_loss: 0.8040 - val_accuracy: 0.7500\n",
      "Epoch 5/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.1001 - accuracy: 0.9626 - val_loss: 0.9649 - val_accuracy: 0.7083\n",
      "Epoch 6/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0785 - accuracy: 0.9732 - val_loss: 0.7391 - val_accuracy: 0.7917\n",
      "Epoch 7/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0684 - accuracy: 0.9751 - val_loss: 0.6955 - val_accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "317fb5c5-0698-4bb0-8384-ab2bfa1df448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2497e18c130>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ada1b6-c58b-4750-8f08-ac18029d0bbf",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "040a2088-3698-4b37-b3d6-10607f11cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36839e61-fab0-46c5-ae96-28ff4cdb51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7af1dd7-f477-4fdb-bde5-4343302683bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "      <th>actual</th>\n",
       "      <th>Pred_proba_Normal</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0001-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0003-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0005-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0006-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0007-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0001-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0003-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0005-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0006-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0007-0001.jpeg   \n",
       "\n",
       "  Category  actual  Pred_proba_Normal  class_pred  \n",
       "0   NORMAL       0           0.049802           1  \n",
       "1   NORMAL       0           0.119644           1  \n",
       "2   NORMAL       0           0.009082           1  \n",
       "3   NORMAL       0           0.002190           1  \n",
       "4   NORMAL       0           0.019208           1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6_pred = df_test.copy()\n",
    "\n",
    "model6_pred['actual'] = model6_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model6_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model6_pred['class_pred'] = model6_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model6_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d2fdceb-bff8-450f-aea1-43b80be3db6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 76, 158],\n",
       "       [  2, 388]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(model6_pred['actual'], model6_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0607882a-c822-4cc1-b526-08697f01b8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7435897435897436"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model6_pred['actual'], model6_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97b3a4-919d-4f1e-915f-317f6b57ec01",
   "metadata": {},
   "source": [
    "#  Model 7\n",
    "- Trying models with repeated CN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11c73-18d5-4ca5-a1cd-4a8e006c331c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8ce2a27-1ac0-4f66-b0a5-b4f65ddbfc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 250\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d384c6-350a-4319-8c7b-fb058899a5a2",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dafd1eef-fcff-4e5f-a09b-654b334728a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2b9e414-8d8d-4df4-89c8-050e583a1930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ec49d43-3b2f-4ef0-859d-607c5331079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d91477eb-f4ea-4325-8fd5-ea83f8a3f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=14)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab96a39-6ef8-4258-b26a-f241ba4824b3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc218383-7f54-4081-b699-5eeeee30d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 250, 250, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 250, 250, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 125, 125, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 31, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7, 7, 512)         33280     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7, 7, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 100354    \n",
      "=================================================================\n",
      "Total params: 729,250\n",
      "Trainable params: 728,834\n",
      "Non-trainable params: 416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9be98fc4-0f50-48b7-a54f-64832603a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "324/324 [==============================] - 32s 97ms/step - loss: 0.2310 - accuracy: 0.9155 - val_loss: 1.8567 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "324/324 [==============================] - 34s 105ms/step - loss: 0.1364 - accuracy: 0.9487 - val_loss: 1.2259 - val_accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.1162 - accuracy: 0.9551 - val_loss: 0.2716 - val_accuracy: 0.8333\n",
      "Epoch 4/100\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.1084 - accuracy: 0.9620 - val_loss: 3.5340 - val_accuracy: 0.6250\n",
      "Epoch 5/100\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.0883 - accuracy: 0.9691 - val_loss: 1.3235 - val_accuracy: 0.7500\n",
      "Epoch 6/100\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.0982 - accuracy: 0.9645 - val_loss: 6.4012 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.0699 - accuracy: 0.9738 - val_loss: 0.3838 - val_accuracy: 0.8125\n",
      "Epoch 8/100\n",
      "324/324 [==============================] - 38s 116ms/step - loss: 0.0645 - accuracy: 0.9778 - val_loss: 0.2646 - val_accuracy: 0.8750\n",
      "Epoch 9/100\n",
      "324/324 [==============================] - 33s 101ms/step - loss: 0.0629 - accuracy: 0.9769 - val_loss: 0.2120 - val_accuracy: 0.8958\n",
      "Epoch 10/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0554 - accuracy: 0.9784 - val_loss: 0.2514 - val_accuracy: 0.8542\n",
      "Epoch 11/100\n",
      "324/324 [==============================] - 30s 91ms/step - loss: 0.0561 - accuracy: 0.9784 - val_loss: 0.1938 - val_accuracy: 0.9167\n",
      "Epoch 12/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0550 - accuracy: 0.9788 - val_loss: 0.7260 - val_accuracy: 0.7917\n",
      "Epoch 13/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0509 - accuracy: 0.9811 - val_loss: 0.1446 - val_accuracy: 0.9583\n",
      "Epoch 14/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 0.1964 - val_accuracy: 0.8958\n",
      "Epoch 15/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0509 - accuracy: 0.9811 - val_loss: 0.1645 - val_accuracy: 0.8958\n",
      "Epoch 16/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0500 - accuracy: 0.9805 - val_loss: 0.1308 - val_accuracy: 0.9375\n",
      "Epoch 17/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0500 - accuracy: 0.9792 - val_loss: 0.1662 - val_accuracy: 0.9375\n",
      "Epoch 18/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0435 - accuracy: 0.9830 - val_loss: 0.6556 - val_accuracy: 0.8333\n",
      "Epoch 19/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0472 - accuracy: 0.9844 - val_loss: 0.3570 - val_accuracy: 0.8542\n",
      "Epoch 20/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0463 - accuracy: 0.9846 - val_loss: 0.1197 - val_accuracy: 0.9583\n",
      "Epoch 21/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0455 - accuracy: 0.9848 - val_loss: 0.1258 - val_accuracy: 0.9375\n",
      "Epoch 22/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0453 - accuracy: 0.9813 - val_loss: 0.1254 - val_accuracy: 0.9375\n",
      "Epoch 23/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0460 - accuracy: 0.9817 - val_loss: 0.1218 - val_accuracy: 0.9375\n",
      "Epoch 24/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0449 - accuracy: 0.9838 - val_loss: 0.1258 - val_accuracy: 0.9375\n",
      "Epoch 25/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0404 - accuracy: 0.9859 - val_loss: 0.1251 - val_accuracy: 0.9375\n",
      "Epoch 26/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0441 - accuracy: 0.9832 - val_loss: 0.1236 - val_accuracy: 0.9375\n",
      "Epoch 27/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0431 - accuracy: 0.9823 - val_loss: 0.1326 - val_accuracy: 0.9375\n",
      "Epoch 28/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 0.1208 - val_accuracy: 0.9375\n",
      "Epoch 29/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0434 - accuracy: 0.9828 - val_loss: 0.1234 - val_accuracy: 0.9375\n",
      "Epoch 30/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0447 - accuracy: 0.9840 - val_loss: 0.1278 - val_accuracy: 0.9375\n",
      "Epoch 31/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0421 - accuracy: 0.9859 - val_loss: 0.1424 - val_accuracy: 0.9375\n",
      "Epoch 32/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0395 - accuracy: 0.9871 - val_loss: 0.1211 - val_accuracy: 0.9375\n",
      "Epoch 33/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0440 - accuracy: 0.9838 - val_loss: 0.1295 - val_accuracy: 0.9375\n",
      "Epoch 34/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0393 - accuracy: 0.9863 - val_loss: 0.1222 - val_accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4024a0f1-1e9d-4d7b-87ea-24a20a882fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24a31dc11c0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c460b-1f1f-4f0d-8727-d9d041fdb724",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9c54933-0fd5-469f-85ba-7e5f4dcae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dee0f936-e67a-4f37-892f-4ef7c2840ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a1bb355-4a6a-4872-aff8-a2b033ff02e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "      <th>actual</th>\n",
       "      <th>Pred_proba_Normal</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0001-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0003-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0005-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0006-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.878912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0007-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0001-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0003-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0005-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0006-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0007-0001.jpeg   \n",
       "\n",
       "  Category  actual  Pred_proba_Normal  class_pred  \n",
       "0   NORMAL       0           0.946312           0  \n",
       "1   NORMAL       0           0.946622           0  \n",
       "2   NORMAL       0           0.973942           0  \n",
       "3   NORMAL       0           0.878912           0  \n",
       "4   NORMAL       0           0.999556           0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7_pred = df_test.copy()\n",
    "\n",
    "model7_pred['actual'] = model7_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model7_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model7_pred['class_pred'] = model7_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model7_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baf88657-de4b-45e7-b2c5-469900940868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163,  71],\n",
       "       [ 16, 374]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(model7_pred['actual'], model7_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30bc5cc8-d355-4268-b885-6cf9a21ad69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8605769230769231"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model7_pred['actual'], model7_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6212e-f103-4afe-9b0d-6c9d5a71c67a",
   "metadata": {},
   "source": [
    "# Model 8\n",
    "- Same as model 7 but with a 128 layer cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3bd42-46be-4138-a7dc-9c9a4bb7226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3df34629-a916-4f5f-99b9-d5546e6454f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 250\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d1810-957f-44da-9159-50b22ea80b99",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7a90dc8-3f36-406e-bdfd-5a8a10369695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a357e432-2f0c-4ad2-ab68-30923fffa781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d1b7a2b-5c14-4d1e-ba2b-b04542376ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4d50d13-5741-4069-b954-aa61e9220166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "early_stop = EarlyStopping(patience=14)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f26b85-3c48-4a42-b223-90eb66bc1326",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1bcc6da5-7cdb-4ca1-bc43-a20d06a9638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 250, 250, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 250, 250, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 125, 125, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 31, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1, 1, 512)         66048     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1, 1, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 886,178\n",
      "Trainable params: 885,250\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67c2f191-f2df-492d-a329-799ee99a329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "324/324 [==============================] - 32s 95ms/step - loss: 0.2608 - accuracy: 0.8993 - val_loss: 5.9948 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.1642 - accuracy: 0.9396 - val_loss: 3.5632 - val_accuracy: 0.6250\n",
      "Epoch 3/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.1263 - accuracy: 0.9551 - val_loss: 0.2967 - val_accuracy: 0.8542\n",
      "Epoch 4/100\n",
      "324/324 [==============================] - 32s 97ms/step - loss: 0.1172 - accuracy: 0.9581 - val_loss: 2.9615 - val_accuracy: 0.6042\n",
      "Epoch 5/100\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.1030 - accuracy: 0.9635 - val_loss: 6.4431 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.1106 - accuracy: 0.9624 - val_loss: 5.4132 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0847 - accuracy: 0.9676 - val_loss: 0.1568 - val_accuracy: 0.9583\n",
      "Epoch 8/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0728 - accuracy: 0.9755 - val_loss: 0.1420 - val_accuracy: 0.9375\n",
      "Epoch 9/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0705 - accuracy: 0.9769 - val_loss: 0.1848 - val_accuracy: 0.8958\n",
      "Epoch 10/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0695 - accuracy: 0.9757 - val_loss: 1.8726 - val_accuracy: 0.7083\n",
      "Epoch 11/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0617 - accuracy: 0.9767 - val_loss: 0.4176 - val_accuracy: 0.8125\n",
      "Epoch 12/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0671 - accuracy: 0.9747 - val_loss: 0.1006 - val_accuracy: 0.9583\n",
      "Epoch 13/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0657 - accuracy: 0.9749 - val_loss: 0.1102 - val_accuracy: 0.9583\n",
      "Epoch 14/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0620 - accuracy: 0.9753 - val_loss: 0.0994 - val_accuracy: 0.9583\n",
      "Epoch 15/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0544 - accuracy: 0.9799 - val_loss: 0.1081 - val_accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0556 - accuracy: 0.9794 - val_loss: 0.0988 - val_accuracy: 0.9583\n",
      "Epoch 17/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0594 - accuracy: 0.9774 - val_loss: 0.1155 - val_accuracy: 0.9583\n",
      "Epoch 18/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0556 - accuracy: 0.9796 - val_loss: 0.0998 - val_accuracy: 0.9583\n",
      "Epoch 19/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0609 - accuracy: 0.9788 - val_loss: 0.1062 - val_accuracy: 0.9583\n",
      "Epoch 20/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0556 - accuracy: 0.9805 - val_loss: 0.1026 - val_accuracy: 0.9583\n",
      "Epoch 21/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0592 - accuracy: 0.9786 - val_loss: 0.0951 - val_accuracy: 0.9583\n",
      "Epoch 22/100\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0580 - accuracy: 0.9786 - val_loss: 0.0909 - val_accuracy: 0.9792\n",
      "Epoch 23/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0602 - accuracy: 0.9767 - val_loss: 0.1000 - val_accuracy: 0.9583\n",
      "Epoch 24/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.1023 - val_accuracy: 0.9583\n",
      "Epoch 25/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0548 - accuracy: 0.9794 - val_loss: 0.0953 - val_accuracy: 0.9583\n",
      "Epoch 26/100\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0639 - accuracy: 0.9770 - val_loss: 0.1061 - val_accuracy: 0.9583\n",
      "Epoch 27/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0602 - accuracy: 0.9778 - val_loss: 0.1137 - val_accuracy: 0.9583\n",
      "Epoch 28/100\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0531 - accuracy: 0.9805 - val_loss: 0.0981 - val_accuracy: 0.9583\n",
      "Epoch 29/100\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.0543 - accuracy: 0.9794 - val_loss: 0.0918 - val_accuracy: 0.9583\n",
      "Epoch 30/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0579 - accuracy: 0.9796 - val_loss: 0.0893 - val_accuracy: 0.9583\n",
      "Epoch 31/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0546 - accuracy: 0.9815 - val_loss: 0.1030 - val_accuracy: 0.9583\n",
      "Epoch 32/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0598 - accuracy: 0.9786 - val_loss: 0.0948 - val_accuracy: 0.9792\n",
      "Epoch 33/100\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.0543 - accuracy: 0.9809 - val_loss: 0.0910 - val_accuracy: 0.9583\n",
      "Epoch 34/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0553 - accuracy: 0.9807 - val_loss: 0.0950 - val_accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0521 - accuracy: 0.9801 - val_loss: 0.0959 - val_accuracy: 0.9583\n",
      "Epoch 36/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0544 - accuracy: 0.9788 - val_loss: 0.0956 - val_accuracy: 0.9583\n",
      "Epoch 37/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0546 - accuracy: 0.9799 - val_loss: 0.0882 - val_accuracy: 0.9792\n",
      "Epoch 38/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0525 - accuracy: 0.9790 - val_loss: 0.0962 - val_accuracy: 0.9583\n",
      "Epoch 39/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0577 - accuracy: 0.9780 - val_loss: 0.0899 - val_accuracy: 0.9583\n",
      "Epoch 40/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0550 - accuracy: 0.9796 - val_loss: 0.1130 - val_accuracy: 0.9583\n",
      "Epoch 41/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0537 - accuracy: 0.9788 - val_loss: 0.0815 - val_accuracy: 0.9792\n",
      "Epoch 42/100\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0538 - accuracy: 0.9805 - val_loss: 0.0886 - val_accuracy: 0.9583\n",
      "Epoch 43/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0471 - accuracy: 0.9844 - val_loss: 0.0853 - val_accuracy: 0.9792\n",
      "Epoch 44/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0609 - accuracy: 0.9782 - val_loss: 0.0875 - val_accuracy: 0.9583\n",
      "Epoch 45/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0467 - accuracy: 0.9828 - val_loss: 0.1025 - val_accuracy: 0.9583\n",
      "Epoch 46/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0490 - accuracy: 0.9809 - val_loss: 0.0824 - val_accuracy: 0.9792\n",
      "Epoch 47/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0475 - accuracy: 0.9817 - val_loss: 0.0857 - val_accuracy: 0.9792\n",
      "Epoch 48/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0469 - accuracy: 0.9824 - val_loss: 0.0861 - val_accuracy: 0.9583\n",
      "Epoch 49/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0507 - accuracy: 0.9803 - val_loss: 0.1086 - val_accuracy: 0.9583\n",
      "Epoch 50/100\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.0558 - accuracy: 0.9794 - val_loss: 0.0895 - val_accuracy: 0.9583\n",
      "Epoch 51/100\n",
      "324/324 [==============================] - 31s 97ms/step - loss: 0.0541 - accuracy: 0.9805 - val_loss: 0.0906 - val_accuracy: 0.9583\n",
      "Epoch 52/100\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0510 - accuracy: 0.9809 - val_loss: 0.0861 - val_accuracy: 0.9792\n",
      "Epoch 53/100\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.0544 - accuracy: 0.9801 - val_loss: 0.0911 - val_accuracy: 0.9583\n",
      "Epoch 54/100\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.0502 - accuracy: 0.9815 - val_loss: 0.0822 - val_accuracy: 0.9583\n",
      "Epoch 55/100\n",
      "324/324 [==============================] - 32s 97ms/step - loss: 0.0567 - accuracy: 0.9770 - val_loss: 0.0868 - val_accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f776df89-b746-4f99-8982-cd83cd4bb00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x249e2b0f760>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f966a8-518a-4261-8148-f5c338ad3df2",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bee65667-9f4a-43dd-823d-513c071257da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a12c4d48-47d2-44b0-a9a5-ca403f7cdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ef4b968-779d-4e26-b328-8d79402506cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "      <th>actual</th>\n",
       "      <th>Pred_proba_Normal</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0001-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0003-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0005-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0006-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0007-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0001-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0003-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0005-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0006-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0007-0001.jpeg   \n",
       "\n",
       "  Category  actual  Pred_proba_Normal  class_pred  \n",
       "0   NORMAL       0           0.999914           0  \n",
       "1   NORMAL       0           0.906134           0  \n",
       "2   NORMAL       0           0.995976           0  \n",
       "3   NORMAL       0           0.994590           0  \n",
       "4   NORMAL       0           0.999805           0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8_pred = df_test.copy()\n",
    "\n",
    "model8_pred['actual'] = model8_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model8_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model8_pred['class_pred'] = model8_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model8_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44591e1d-7655-40b1-a5b5-1a8bc4f6a456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[186,  48],\n",
       "       [ 16, 374]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(model8_pred['actual'], model8_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1ff666f-bb13-4379-9245-d6b75381bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8974358974358975"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model8_pred['actual'], model8_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e710b34e-c6b0-4e2b-a3b4-340d355b293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = model.save('model_8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a3e41c-a3ee-49fa-933f-75cd9aa6c46f",
   "metadata": {},
   "source": [
    "#### Ok so this was the best performing model. About 90% accuracy\n",
    "- However it took too long to train because of early stoping\n",
    "- Will use a max of 33 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a89c50-0570-47e0-8e0e-1b3911b22973",
   "metadata": {},
   "source": [
    "# Model 9\n",
    "- Will remove early stoping and put 33 epochs\n",
    "- will increase compexity (cnn of 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bad42f-34c8-4bd3-b840-f7d5de9af561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4905c6-1572-44cc-bafc-1202116f8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 250\n",
    "input_shape = (image_size,image_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60ba74-9536-43b2-957a-457c3027db16",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7b6545-211b-4943-9b0e-a9ab8fb9f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255,\n",
    "                           rotation_range=15,\n",
    "                           zoom_range=0.05,\n",
    "                           horizontal_flip=True,\n",
    "                          )\n",
    "\n",
    "train_data = train.flow_from_directory(train_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ace613f-4b7e-41e5-ad6f-06aad80a70d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = valid.flow_from_directory(val_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1328201b-296b-4554-a6da-90ec2e6bf797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_data = test.flow_from_directory(test_dir,\n",
    "                                       target_size=(image_size,image_size),\n",
    "                                       color_mode='grayscale',\n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7644da-ea30-46df-8825-ebdf8db93d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining call backs\n",
    "#early_stop = EarlyStopping(patience=14)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.00001,)\n",
    "\n",
    "\n",
    "callbacks = [reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7b7c0-8aac-463c-8f5e-edb5f593cf6f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b12d57c-7ad2-49f5-851f-01c616208436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 250, 250, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 250, 250, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 125, 125, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 31, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 15, 15, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3, 3, 512)         131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3, 3, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3, 3, 2048)        2099200   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 36866     \n",
      "=================================================================\n",
      "Total params: 3,973,026\n",
      "Trainable params: 3,971,074\n",
      "Non-trainable params: 1,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(256,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(256,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56211cf4-7647-4cc4-bdfc-f9a6de100cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/34\n",
      "324/324 [==============================] - 37s 96ms/step - loss: 0.2704 - accuracy: 0.8970 - val_loss: 3.9717 - val_accuracy: 0.5000\n",
      "Epoch 2/34\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.1588 - accuracy: 0.9466 - val_loss: 7.6782 - val_accuracy: 0.5625\n",
      "Epoch 3/34\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.1571 - accuracy: 0.9444 - val_loss: 0.4199 - val_accuracy: 0.8125\n",
      "Epoch 4/34\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.1232 - accuracy: 0.9589 - val_loss: 11.9436 - val_accuracy: 0.5208\n",
      "Epoch 5/34\n",
      "324/324 [==============================] - 31s 97ms/step - loss: 0.1082 - accuracy: 0.9612 - val_loss: 0.4167 - val_accuracy: 0.8750\n",
      "Epoch 6/34\n",
      "324/324 [==============================] - 31s 95ms/step - loss: 0.1018 - accuracy: 0.9585 - val_loss: 0.5108 - val_accuracy: 0.7500\n",
      "Epoch 7/34\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.1012 - accuracy: 0.9643 - val_loss: 3.5020 - val_accuracy: 0.7083\n",
      "Epoch 8/34\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0918 - accuracy: 0.9672 - val_loss: 10.4375 - val_accuracy: 0.5000\n",
      "Epoch 9/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0637 - accuracy: 0.9767 - val_loss: 0.4736 - val_accuracy: 0.7917\n",
      "Epoch 10/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0574 - accuracy: 0.9790 - val_loss: 0.6144 - val_accuracy: 0.7708\n",
      "Epoch 11/34\n",
      "324/324 [==============================] - 31s 96ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 1.0575 - val_accuracy: 0.7917\n",
      "Epoch 12/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0457 - accuracy: 0.9828 - val_loss: 0.5533 - val_accuracy: 0.8125\n",
      "Epoch 13/34\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0541 - accuracy: 0.9811 - val_loss: 0.5169 - val_accuracy: 0.8125\n",
      "Epoch 14/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0493 - accuracy: 0.9807 - val_loss: 0.4900 - val_accuracy: 0.8542\n",
      "Epoch 15/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0498 - accuracy: 0.9817 - val_loss: 0.5057 - val_accuracy: 0.8125\n",
      "Epoch 16/34\n",
      "324/324 [==============================] - 32s 98ms/step - loss: 0.0486 - accuracy: 0.9805 - val_loss: 0.4501 - val_accuracy: 0.8542\n",
      "Epoch 17/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0489 - accuracy: 0.9809 - val_loss: 0.4983 - val_accuracy: 0.7917\n",
      "Epoch 18/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0525 - accuracy: 0.9797 - val_loss: 0.4946 - val_accuracy: 0.7917\n",
      "Epoch 19/34\n",
      "324/324 [==============================] - 30s 94ms/step - loss: 0.0487 - accuracy: 0.9832 - val_loss: 0.4764 - val_accuracy: 0.8542\n",
      "Epoch 20/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0510 - accuracy: 0.9805 - val_loss: 0.4800 - val_accuracy: 0.8542\n",
      "Epoch 21/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0473 - accuracy: 0.9819 - val_loss: 0.5060 - val_accuracy: 0.8125\n",
      "Epoch 22/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0514 - accuracy: 0.9805 - val_loss: 0.4880 - val_accuracy: 0.8333\n",
      "Epoch 23/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0453 - accuracy: 0.9823 - val_loss: 0.4380 - val_accuracy: 0.8542\n",
      "Epoch 24/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0453 - accuracy: 0.9807 - val_loss: 0.4883 - val_accuracy: 0.8542\n",
      "Epoch 25/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0470 - accuracy: 0.9821 - val_loss: 0.4608 - val_accuracy: 0.8542\n",
      "Epoch 26/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0482 - accuracy: 0.9819 - val_loss: 0.4581 - val_accuracy: 0.8333\n",
      "Epoch 27/34\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0495 - accuracy: 0.9828 - val_loss: 0.4956 - val_accuracy: 0.8125\n",
      "Epoch 28/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0534 - accuracy: 0.9788 - val_loss: 0.5000 - val_accuracy: 0.8333\n",
      "Epoch 29/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0457 - accuracy: 0.9809 - val_loss: 0.5255 - val_accuracy: 0.8333\n",
      "Epoch 30/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0479 - accuracy: 0.9807 - val_loss: 0.5062 - val_accuracy: 0.8750\n",
      "Epoch 31/34\n",
      "324/324 [==============================] - 31s 94ms/step - loss: 0.0471 - accuracy: 0.9828 - val_loss: 0.4940 - val_accuracy: 0.8542\n",
      "Epoch 32/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0478 - accuracy: 0.9821 - val_loss: 0.5059 - val_accuracy: 0.8542\n",
      "Epoch 33/34\n",
      "324/324 [==============================] - 30s 93ms/step - loss: 0.0471 - accuracy: 0.9815 - val_loss: 0.4861 - val_accuracy: 0.8542\n",
      "Epoch 34/34\n",
      "324/324 [==============================] - 30s 92ms/step - loss: 0.0472 - accuracy: 0.9821 - val_loss: 0.5072 - val_accuracy: 0.8542\n"
     ]
    }
   ],
   "source": [
    "epochs = 34\n",
    "history = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_data,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6827e008-d9ed-496e-81df-6d12b5804785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26bf38aebb0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a074a-5d87-4044-90a6-9372eb7d3bb8",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724ab139-5ae5-4131-b129-168f2df6e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1f63dd-b995-420a-99b9-eafc3a197e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index_pred = []\n",
    "for l in pred:\n",
    "    zero_index_pred.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eca731a-95f4-45ad-84be-cdf869293bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "      <th>actual</th>\n",
       "      <th>Pred_proba_Normal</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0001-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0003-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0005-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.713298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0006-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...</td>\n",
       "      <td>IM-0007-0001.jpeg</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.986675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths               File  \\\n",
       "0  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0001-0001.jpeg   \n",
       "1  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0003-0001.jpeg   \n",
       "2  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0005-0001.jpeg   \n",
       "3  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0006-0001.jpeg   \n",
       "4  C:\\Users\\Amit\\Desktop\\ML Projects\\Chest X-ray ...  IM-0007-0001.jpeg   \n",
       "\n",
       "  Category  actual  Pred_proba_Normal  class_pred  \n",
       "0   NORMAL       0           0.977037           0  \n",
       "1   NORMAL       0           0.838172           0  \n",
       "2   NORMAL       0           0.713298           0  \n",
       "3   NORMAL       0           0.866755           0  \n",
       "4   NORMAL       0           0.986675           0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9_pred = df_test.copy()\n",
    "\n",
    "model9_pred['actual'] = model9_pred['Category'].apply(lambda x: 0 if x=='NORMAL' else 1)\n",
    "model9_pred['Pred_proba_Normal'] = zero_index_pred\n",
    "model9_pred['class_pred'] = model9_pred['Pred_proba_Normal'].apply(lambda x: 1 if x<0.5 else 0)\n",
    "\n",
    "model9_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5e03880-a5e0-4c9b-929e-9fef061c019c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145,  89],\n",
       "       [  5, 385]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(model9_pred['actual'], model9_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f32926b2-ca47-4e0d-83e3-611770a529ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493589743589743"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model9_pred['actual'], model9_pred['class_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8598cfed-310a-48ee-97d1-2aa57c597b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_9 = model.save('model_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259603da-80e0-4b3a-88a1-e16b2e4b8b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
